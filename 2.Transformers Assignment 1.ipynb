{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2594ca79",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Transformers on IMDB Dataset\n",
    "\n",
    "This notebook performs sentiment analysis on the IMDB dataset using Hugging Face Transformers. We use `distilbert-base-uncased` and `bert-base-uncased` for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233849b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset from TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds_train, ds_test = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using Hugging Face Tokenizers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME_1 = \"distilbert-base-uncased\"\n",
    "MODEL_NAME_2 = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(MODEL_NAME_1)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL_NAME_2)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def tokenize_fn(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example.numpy().decode('utf-8'),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "def wrap_tokenizer(tokenizer):\n",
    "    def tf_tokenizer(text, label):\n",
    "        result = tf.py_function(lambda x: tokenize_fn(x, tokenizer), [text], \n",
    "                                Tout={'input_ids': tf.int32, 'attention_mask': tf.int32})\n",
    "        result['label'] = label\n",
    "        return result\n",
    "    return tf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a93d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TensorFlow Datasets\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepare_dataset(ds, tokenizer):\n",
    "    ds = ds.map(wrap_tokenizer(tokenizer), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.shuffle(1000).batch(32).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds1 = prepare_dataset(ds_train, tokenizer1)\n",
    "test_ds1 = prepare_dataset(ds_test, tokenizer1)\n",
    "\n",
    "train_ds2 = prepare_dataset(ds_train, tokenizer2)\n",
    "test_ds2 = prepare_dataset(ds_test, tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compile models\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(model_name):\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    optimizer = Adam(learning_rate=2e-5)\n",
    "    model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model1 = build_model(MODEL_NAME_1)\n",
    "model2 = build_model(MODEL_NAME_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "model1.fit(train_ds1, validation_data=test_ds1, epochs=2)\n",
    "model2.fit(train_ds2, validation_data=test_ds2, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare\n",
    "loss1, acc1 = model1.evaluate(test_ds1)\n",
    "loss2, acc2 = model2.evaluate(test_ds2)\n",
    "\n",
    "print(f\"DistilBERT Accuracy: {acc1:.4f}\")\n",
    "print(f\"BERT Accuracy: {acc2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180a500",
   "metadata": {},
   "source": [
    "### ðŸ“Š Results & Comparison\n",
    "\n",
    "| Model              | Accuracy | Model Size | Speed       |\n",
    "|-------------------|----------|------------|-------------|\n",
    "| DistilBERT         | ~0.88â€“0.89 | Lightweight | Faster       |\n",
    "| BERT-base-uncased  | ~0.91â€“0.92 | Larger      | Slower       |\n",
    "\n",
    "**Conclusion:** BERT gives better performance but at a computational cost. DistilBERT is a great trade-off when speed is important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
